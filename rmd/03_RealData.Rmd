---
title: "Real Data and Descriptive Statistics"
author: "Thomas Manke"
date:  "`r date() `"
output: html_document
---

# Famous Data Sets
R has a large range of pre-defined data sets. They are frequently used to illustrate the functionality of code and software packages. Just type "data()" to get an overview. Let's first focus on the "iris" data set

![Flower Measurements](../images/sepal_petal.jpeg) 
(Image from blog of mathieu.guillame-bert.com)
```{r}
?iris
str(iris)
head(iris)
```
**Task**: explore this data set in terms of data structures and observables.
How many rows and columns does this data frame have? Of which type and class. How do you access the data for all petal lengths?

# Descriptive Statistics
```{r}
summary(iris)
plot(iris$Petal.Length) # a plot at last. Simple, but many arguments: ?plot 
hist(iris$Petal.Length) # and a histogram
```

**Task** hist() can have many arguments. Use help to find out how the histogram can be customized (e.g. number of break points, title, colors). Try some of it.

# Boxplot: a more condensed summary
```{r}
boxplot(iris$Petal.Length)
```

**Task**: The boxplot above is for *all* data. Create a boxplot of petal length for species "setosa" only.
```{r, echo=FALSE}
boxplot(iris$Petal.Length[iris$Species=="setosa"], xlab="setosa", ylab="Petal Length", col="red")
```

Boxplot understands data frames
```{r}
boxplot(iris) # What does the boxplot for Species mean?
```

#Interlude: Factors = categorical variables
Factors denote a special class of R-objects that can be thought of as categories (here: species). They have a given number of *levels* which are internally represented as integers.
```{r}
class(iris$Species)
typeof(iris$Species)
ts=table(iris$Species)  # returns a contigency table ~> histogram for categorical data
barplot(ts, col=rainbow(3), ylab="observations", cex.names=0.9)
pie(ts,col=rainbow(3))
```

***

Boxplot understands factors in data frames
```{r}
boxplot( Petal.Length ~ Species, data = iris, las=2) # what does las=2 do ?
```

**Task**: Use help to add three different colors:
```{r, echo=FALSE}
# use help to determine how to add colors
cols=c("red","yellow","blue")
boxplot( Petal.Length ~ Species, data = iris, las=2,col=cols)
```


# Correlations
If a data set has many numerical variables we often want to understand their correlations structure
```{r}
x=iris$Petal.Length
y=iris$Petal.Width
plot(x,y)                              # again: this can be customized
abline(v=mean(x),h=mean(y),col="red")  # add vertical/horizontal lines
cor(x,y)                               # a correlation coefficient: which one?
```

# All-Against-All Correlations
**Task**: remove the Species variable from "iris" and store the result in a new data.frame "niris"
```{r, echo=FALSE}
niris=iris[,-5]  # generate new data frame without species variable
str(niris)
```

```{r}
cor(niris)   # correlation matrix. Which correlation coefficient?
pairs(niris) # provide a visualization, try also plot(iris)

# assign species-colors to each observation 
cols = iris$Species                        # understand how color is defined
pairs(niris, col=cols, lower.panel=NULL)   # "cols" was defined in task above
```

***

# From Correlations to Models

Goal: Model some dependent variables y as function of other explanatory variables x (features)

$y = f(\theta, x) = \theta_1 x +  \theta_0$

For $N$ data points, choose parameters $\theta$ by ordinary least squares: 

$RSS=\sum_{i=1}^{N} (y_i - y(\theta, x_i))^2 \to min$


```{r}
plot(Petal.Width ~ Petal.Length, data=iris, col=Species) # use model ("formula") notation
fit=lm(Petal.Width ~ Petal.Length, data=iris)       # fit a linear model
abline(fit, lwd=3, lty=2)                           # add regression line
```

**Task**: What kind of class / data type is the object "fit"? Extract the coefficients of the fitted line and determine the residual degrees of freedom.

```{r, echo=FALSE}
fit$coefficients
fit$df.residual
```

#Reporting the fit (model)
```{r}
coefficients(fit)
confint(fit)   # Try to change the confidence level: ?confint
summary(fit)
```

This is a good fit - as suggested by a small residual standard error, a large coefficient of variation $R^2 \in (0,1)$, a small p-value, (and by visualization).  

$R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_i(y_i - y(\theta,x_i))^2}{\sum_i(y_i-\bar{y})^2}$


# Plotting predictions with confidence intervals
```{r}
x=iris$Petal.Length                       # explanatory variable from fit (here:Petal.Length)
xn=seq(min(x), max(x), length.out = 100)  # define range of new explanatory variables
ndf=data.frame(Petal.Length=xn)           # put them into new data frame

p=predict(fit, ndf, interval = 'confidence' , level = 0.95)
plot(Petal.Width ~ Petal.Length, data=iris, col=Species)
lines(xn, p[,"lwr"] )
lines(xn, p[,"upr"] )

#some fancy filling
#polygon(c(rev(xn), xn), c(rev(p[ ,"upr"]), p[ ,"lwr"]), col = rgb(1,0,0,0.5), border = NA)

## using ggplot2 - full introduction later
library(ggplot2)
g = ggplot(iris, aes(Petal.Length, Petal.Width, colour=Species))
g + geom_point() + geom_smooth(method="lm", se=TRUE, color="red") + geom_smooth(method="loess", colour="blue")
```

# Example of a poor fit (replace "Petal" with "Sepal)
```{r}
plot(Sepal.Width ~ Sepal.Length, data=iris, col=cols)  
fit1=lm(Sepal.Width ~ Sepal.Length, data=iris)     
abline(fit1, lwd=3, lty=2)    
confint(fit1)                     # estimated slope is indistinguishable from zero
summary(fit1)
```
*Interpretation*: slope is not significantly distinct from 0.

# Run predictions:
```{r}
x=iris$Sepal.Length                       # explanatory variable from fit (here:Sepal.Length)
xn=seq(min(x), max(x), length.out = 100)  # define range of new explanatory variables
ndf=data.frame(Sepal.Length=xn)           # put them into data frame

p=predict(fit1, ndf, interval = 'confidence' , level = 0.95)  # predict values

plot(Sepal.Width ~ Sepal.Length, data=iris, col=Species)
lines(xn, p[,"lwr"] )
lines(xn, p[,"upr"] )
```




# Factorial variables as predictors
In the iris example the "Species" variable is a factorial (categorical) variable with 3 levels.
Other typical examples: different experimental conditions or treatments.

```{r}
plot(Petal.Width ~ Species, data=iris)
fit=lm(Petal.Width ~ Species, data=iris)
summary(fit)
```
*Interpretation*:

"setosa" (1st species) has mean Petal.Width=0.246(29). This is significantly different from 0 (p-value small. useless)
"versicolor" (2nd species) has mean Petal.Width = Petal.Width(setosa) + 1.08(4) which is significantly larger (t~26. tiny p-value). there is a stat. significant difference between groups/species.
"virginica" (3rd species) has mean Petal.Width = Petal.Width(setosa) + 1.78(4)

# More complicated models
Determine residual standard error for different fits with various complexity
```{r}
fit=lm(Petal.Width ~ Petal.Length, data=iris)
paste(toString(fit$call), sigma(fit))
fit=lm(Petal.Width ~ Petal.Length + Sepal.Length, data=iris)  # function of more than one variable
paste(toString(fit$call), sigma(fit))
fit=lm(Petal.Width ~ Species, data=iris)                      # function of categorical variables
paste(toString(fit$call), sigma(fit))
fit=lm(Petal.Width ~ . , data=iris)                          # function of all other variable (numerical and categorical)
paste(toString(fit$call), sigma(fit))
```

... more complex models tend to have smaller residual standard error -> "model selection" (AIC)


# Anova
summary(fit) contains information on the individual coefficients. They are difficult 

```{r}
fit = lm(Petal.Width ~ Petal.Length + Sepal.Width + Sepal.Length + Species , data=iris) 
summary(fit)
```

Question: Rather than looking at differences between different factor levels. Does the factor "Species" as a whole account for variation in the observed Petal.Width. Sum-of-squared analysis: Anova  
```{r}
anova(fit) 

# order of variable matters: the following is not the same
# anova( lm(Petal.Width ~ . , data=iris) )
```
*Interpretation*: Species account for much variation in the data (F(2, 147)=960. p tiny)


**Task**: Repeat the linear regression Petal.Width ~ Petal.Length with an *outlier* in the data 
```{r}
irisout=rbind(iris,list(5.8,3, 4, 20, "virginica"))
```

```{r, echo=FALSE}
str(irisout)
summary(irisout)
plot(Petal.Width ~ Petal.Length, data=irisout, col=Species)
fit2=lm(Petal.Width ~ Petal.Length, data=irisout)
abline(fit2, lwd=3, lty=2)    
summary(fit2)
```

# Diagnostic Plots
"fit" is a large object of the lm-class which contains also lots of diagnostic informmation. Notice how the behaviour of "plot" changes.
```{r}
op=par(no.readonly=TRUE)  # safe only resettable graphical parameters, avoids many warnings
par(mfrow=c(2,2))         # change graphical parameters: 2x2 images on device
plot(fit2,col=irisout$Species)       # four plots rather than one
par(op)                   # reset graphical parameters
```
more examples here: http://www.statmethods.net/stats/regression.html

Linear models $y_i=\theta_0 + \theta_1  x_i + \epsilon_i$ make certain assumptions ($\epsilon_i \propto N(0,\sigma^2)$)

* residuals $\epsilon_i$ are independent from each other (non-linear patterns?)
* residuals are normally distributed
* have equal variance $\sigma^2$ (homoscedascity)
* are there outliers (large residuals) or observations with strong influence on fit

***
# Review:
* use and understand frequently used data sets (iris)
* summary  for descriptive statistics
* plot() for X-Y plots and overloading for other classes (lm)
* hist(), boxplot()
* customize arguments: line width, colours, ...
* correlations: cor()
* linear model lm(): fitting, summary and interpretation
* Notice that the data used was extremely clean and structured: data()
